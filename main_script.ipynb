{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdGIh2c2Kx78"
   },
   "source": [
    "# Assignment 1 - Mini Batch SGD for Linear Image Classification\n",
    "\n",
    "\n",
    "Name: George Paterakis\n",
    "AM : 1247\n",
    "\n",
    "\n",
    "In this exercise you will:\n",
    "    \n",
    "- implement a **loss function** for a linear classifier\n",
    "- **optimize** the loss function with **SGD**\n",
    "- use a validation set to **tune the hyperparameter (learning rate, regularization strength, regularization type, mini batch size.)**\n",
    "- **visualize** the final learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wtFDEUCKx8A"
   },
   "source": [
    "# Download your dataset\n",
    "\n",
    "\n",
    "Before starting you should download and set your dataset.\n",
    "\n",
    "1) Download from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "\n",
    "2) Extract the .tar.gz file into your assignment1/datasets folder\n",
    "\n",
    "3) Check that the 8 files of the dataset are in the folder **assignment1/datasets/cifar-10-batches-py/**\n",
    "\n",
    "4) You may find useful information about the dataset in the readme.html of that folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "id": "qBzEL7gxKx8A",
    "outputId": "6ce85ede-92d0-4017-97ef-10fe95d61171"
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from classUtils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This makes matplotlib figures appear inline in the\n",
    "# notebook rather than in a new window.\n",
    "#%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrW1c5yIKx8C"
   },
   "source": [
    "## CIFAR-10 Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "id": "07rqVE2GKx8C",
    "outputId": "6ccae230-8610-4133-c84f-070bdcf7fce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (50000L, 32L, 32L, 3L)\n",
      "Training labels shape:  (50000L,)\n",
      "Test data shape:  (10000L, 32L, 32L, 3L)\n",
      "Test labels shape:  (10000L,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print 'Training data shape: ', X_train.shape\n",
    "print 'Training labels shape: ', y_train.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "id": "WxGmAT71Kx8D",
    "outputId": "92507aee-99de-4929-ac1d-0266e99c8160"
   },
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "id": "qeWI8QF3Kx8D",
    "outputId": "878d04f4-f1f1-40a1-b016-780906b17df0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000L, 32L, 32L, 3L)\n",
      "Train labels shape:  (49000L,)\n",
      "Validation data shape:  (1000L, 32L, 32L, 3L)\n",
      "Validation labels shape:  (1000L,)\n",
      "Test data shape:  (1000L, 32L, 32L, 3L)\n",
      "Test labels shape:  (1000L,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train, val, and test sets. In addition we will\n",
    "# create a small development set as a subset of the training data;\n",
    "# we can use this for development so our code runs faster.\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "mask = range(num_test)\n",
    "X_test = X_test[mask]\n",
    "y_test = y_test[mask]\n",
    "\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "id": "ai5jtwAgKx8E",
    "outputId": "12866a97-8c8e-493f-b2e0-ec84d75727c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (49000L, 3072L)\n",
      "Validation data shape:  (1000L, 3072L)\n",
      "Test data shape:  (1000L, 3072L)\n",
      "dev data shape:  (500L, 3072L)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "# As a sanity check, print out the shapes of the data\n",
    "print 'Training data shape: ', X_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'dev data shape: ', X_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "id": "4Pvrce4TKx8E",
    "outputId": "53519b9b-e819-42a9-a2a5-c16887fce48a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 130.64189796  135.98173469  132.47391837  130.05569388  135.34804082\n",
      "  131.75402041  130.96055102  136.14328571  132.47636735  131.48467347]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: subtract the mean image\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "print mean_image[:10] # print a few of the elements\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "id": "8kZnKWpqKx8F"
   },
   "outputs": [],
   "source": [
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "id": "5MtnV0z3Kx8G",
    "outputId": "7d7ab371-f8a9-4ea4-f143-71bc9e1f3b03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000L, 3073L) (1000L, 3073L) (1000L, 3073L) (500L, 3073L)\n"
     ]
    }
   ],
   "source": [
    "# third: append the bias dimension of ones (i.e. bias trick) so that our classifier\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print X_train.shape, X_val.shape, X_test.shape, X_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fIaJOt1Kx8G"
   },
   "source": [
    "## 1. Stochastic Gradient Descent\n",
    "\n",
    "Your code for this section will all be written inside **compute_gradient_and_loss.py**.\n",
    "\n",
    "-As a ﬁrst step, you will need to correctly fill-in the method 'compute_gradient_and_loss' that takes as input a set of training samples and computes the loss and the gradient of the loss (for the given training samples). \n",
    "\n",
    "-You will call this function inside the **train_linear_classifer method** of the **LinearClassifier Class** in the  **linear_classifier.py** file in order to compute the gradient of each mini-batch, and for collecting the sequence of all mini-batch losses during training as well as the sequence of all validation losses during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "id": "Gqcxs31YKx8H",
    "outputId": "db13c9a6-6ff1-48c8-850c-b89506594318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.69193570e-05  -6.63756409e-05  -6.20447310e-05 ...,   9.59235504e-05\n",
      "   3.85428466e-05   1.39833838e-05]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3072,) and (3073,) not aligned: 3072 (dim 0) != 3073 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1f16785059b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3073\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_gradient_and_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'loss: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Desktop\\587\\cs587_assignment1\\classUtils\\classifiers\\compute_gradient_loss.py\u001b[0m in \u001b[0;36mcompute_gradient_and_loss\u001b[1;34m(W, X, y, reg, reg_type, opt)\u001b[0m\n\u001b[0;32m     41\u001b[0m       \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW_T\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0msample_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mf_pred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW_T\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\George\\Desktop\\587\\cs587_assignment1\\classUtils\\classifiers\\compute_gradient_loss.py\u001b[0m in \u001b[0;36mf_pred\u001b[1;34m(weights, features)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mf_pred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (3072,) and (3073,) not aligned: 3072 (dim 0) != 3073 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Check that the implementation of the compute_gradient_and_loss function is ok by calling it directly using random W as input.\n",
    "from classUtils.classifiers.compute_gradient_loss import compute_gradient_and_loss\n",
    "import time\n",
    "\n",
    "# generate a random classifier weight matrix of small numbers\n",
    "W = np.random.randn(3073, 10) * 0.0001 \n",
    "\n",
    "loss, grad = compute_gradient_and_loss(W, X_dev, y_dev, 0.00001, 2, 0)\n",
    "print 'loss: %f' % (loss, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T1c8h01Kx8H"
   },
   "source": [
    "## 2. Implement your linear classifier\n",
    "\n",
    "To implement your linear classifier, you will need to fill-in the following\n",
    "two functions: \n",
    "\n",
    "'train_linear_classifier': this is the method of class LinearClassifier responsible for training the\n",
    "classiﬁer using mini-batch SGD. It should return the parameters of the\n",
    "trained classiﬁer and the sequence of all mini-batch losses during training\n",
    "as well as the sequence of all validation losses during training.\n",
    "\n",
    "'predict_image_class': this is the method of class LinearClassifier  takes as input an image and uses a\n",
    "trained classiﬁer to predict its class (recall that the predicted class should\n",
    "be the one that is assigned the maximum score by the trained classifer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "zNWrec0IKx8H",
    "outputId": "bcc017e4-a2e6-4b67-915d-a9fa949bd649"
   },
   "outputs": [],
   "source": [
    "# In the file linear_classifier.py, implement SGD in the function\n",
    "# LinearClassifier.train_linear_classifier() and then run it with the code below.\n",
    "# Plot the loss of the training process as a function of iteration number.\n",
    "\n",
    "from classUtils.classifiers import LinearClassifier\n",
    "cifarLC = LinearClassifier()\n",
    "tic = time.time()\n",
    "\n",
    "loss_hist, val_loss_hist = cifarLC.train_linear_classifier(X_train, y_train, X_val, y_val, learning_rate=1e-7, reg=5e4, \\\n",
    "                                        reg_type = 2, num_epochs=6, batch_size = 200, num_valid_loss_evals = 100, verbose=True)\n",
    "toc = time.time()\n",
    "print 'Time elapsed: %f secs' % (toc - tic)\n",
    "\n",
    "# A useful debugging strategy is to plot the loss as a function of iteration number !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "-KCE8mFVKx8I",
    "outputId": "b2bc9eed-4944-4fa8-f52b-b915c7822995",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Implement the LinearClassifier.predict_image_class function and evaluate the performance on both the\n",
    "# training and validation set\n",
    "y_train_pred = cifarLC.predict_image_class(X_train)\n",
    "print 'training accuracy: %f' % (np.mean(y_train == y_train_pred), )\n",
    "\n",
    "y_val_pred = cifarLC.predict_image_class(X_val)\n",
    "print 'validation accuracy: %f' % (np.mean(y_val == y_val_pred), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZcZWTHRKx8I"
   },
   "source": [
    "### 3. Choose the best hyperparameters using the validation set\n",
    "\n",
    "You will use the validation set in order to choose proper values for some of the hyperparameters of the problem \n",
    "(these include the regularization strength, the mini-batch size, learning rate and the type of regularization l1 or l2). \n",
    "\n",
    "To that end, you will train linear classiﬁers for a diﬀerent number of combinations of these hyperparameters\n",
    "and you will choose as your ﬁnal classiﬁer the one that achieves the highest accuracy in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "pOUAwY1lKx8I",
    "outputId": "45f28129-230c-4ef2-fce2-4d8ca9d4bc46"
   },
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization type and strength, learning rate and batch size). \n",
    "# You can run your experiments using the following 8 combinations (columnwise) \n",
    "# You are encouraged to use your own combinations on different ranges of the hyperparameters to achieve the highest accuracy.\n",
    "# If you are careful you should be able to get a classification accuracy of about 0.4 on the validation set.\n",
    "learning_rates          = [1e-8, 1e-7, 3e-7, 3e-7, 5e-7, 8e-7, 1e-6, 1e-5]\n",
    "regularization_strengths= [1e4,  3e4,  5e4,   1e4, 8e4,  1e5,  5e4,  5e5 ]\n",
    "regularization_type     = [1,      2,    1,     2,   1,    2,    1,    2 ] # 1,2 for l1, l2 respectively\n",
    "batch_size              = [50,   100,  200,   400, 100,  200,  200,  400 ]\n",
    "num_epochs = 6\n",
    "\n",
    "# results is a container for saving the results of your cross-validation\n",
    "# HINT : you can use a dictionary for mapping tuples of the form\n",
    "# (learning_rate, regularization_strength, regularization_type, batch_size) \n",
    "# to tuples of the form (training_accuracy, validation_accuracy). \n",
    "# The accuracy is simply the fraction of data points that are correctly classified.\n",
    "results = []\n",
    "best_train_val = -1   # The highest training accuracy that we have seen so far.\n",
    "best_valid_val = -1   # The highest validation accuracy that we have seen so far.\n",
    "best_classifier = None # The LinearClassifier object that achieved the highest validation rate.\n",
    "best_lr = best_reg = best_reg_type = best_batch_size = 0\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For some combinations of hyperparameters, train a linear clasifier on   #\n",
    "# the training set, compute its accuracy on the training and validation sets,  #\n",
    "# store these numbers in the results dictionary. In addition, store the best   #\n",
    "# validation accuracy in best_val and the LinearClassifier object that achieves#\n",
    "# this accuracy in best_classifier.                                            #\n",
    "# !!! Also, print out or plot the resulting accuracy for the selected          #\n",
    "# combinations of your hyperparameters.                                        #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "print 'best training and validation accuracy achieved during cross-validation: %f and %f' % (best_train_val,best_valid_val)\n",
    "print 'using parameters: lr %e reg %e reg_type %d and batch_size %d' % (best_lr, best_reg, best_reg_type, best_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vTuaGHhKx8I"
   },
   "source": [
    "### 4. Test your best classifier and visualize the learnt weights\n",
    "\n",
    "For the ﬁnal classiﬁer, you should \n",
    "\n",
    "1) draw (in the same plot) the sequence of mini-batch losses and validation losses  collected during training. \n",
    "\n",
    "2) Evaluate the classiﬁer on the test set and report the achieved test accuracy\n",
    "\n",
    "3) visualize (as images) the weights W (one image per row of W)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8XIsOGGkKx8J"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:  Get the mini-batch training losses and validation losses collected    #\n",
    "# during training of your best classifier and plot in the same plot            #\n",
    "################################################################################\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "XLT7HtxvKx8J"
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# TODO:  Evaluate the best_classifier on the test set and plot/print the accuracy #\n",
    "###################################################################################\n",
    "test_accuracy = 0\n",
    "\n",
    "print 'linear classifier on raw pixels final test set accuracy: %f' % test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "L3PU4EMHKx8J"
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class.\n",
    "#IF you have calculated valid W weights just the following routine will visualize the learned weights\n",
    "#Just run the following lines of code\n",
    "\n",
    "w = best_classifier.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "main_script.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "23b41bcac9c042858dd072025ea01c4bd4f9bc002cab94b3d92c165359c30ed2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
